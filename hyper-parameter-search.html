

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Hyper Parameter Search &mdash; dask-ml 1.1.2 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript" src="_static/js/custom.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/style.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/explore.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/nbsphinx.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Pipelines and Composite Estimators" href="compose.html" />
    <link rel="prev" title="Cross Validation" href="cross_validation.html" />
  <link rel="shortcut icon" href="_static/images/favicon.ico"/>

</head>

<body class="wy-body-for-nav">

  

<nav id="explore-links">
  <a href="https://docs.dask.org/">
    <img class="caption" src="_static/images/dask-horizontal-white.svg"/>
  </a>

  <ul>
    <li>
      <a>Get Started</a>
      <ul>
        <li><a href="https://docs.dask.org/en/latest/install.html"> Install </a></li>
        <li><a href="https://examples.dask.org"> Examples </a></li>
        <li><a href="https://github.com/dask/dask-tutorial"> Tutorial </a></li>
        <li><a href="https://docs.dask.org/en/latest/why.html"> Why Dask? </a></li>
        <li><a href="https://stories.dask.org/en/latest"> Use Cases </a></li>
        <li><a href="https://www.youtube.com/watch?v=RA_2qdipVng&list=PLRtz5iA93T4PQvWuoMnIyEIz1fXiJ5Pri"> Talks </a></li>
        <li><a href="https://mybinder.org/v2/gh/dask/dask-examples/master?urlpath=lab"> Try Online </a></li>
        <li><a href="https://dask.org/slides"> Slides </a></li>
      </ul>
    </li>

    <li>
      <a href="">Algorithms</a>
      <ul>
        <li><a href="https://docs.dask.org/en/latest/array.html">Arrays</a></li>
        <li><a href="https://docs.dask.org/en/latest/dataframe.html">Dataframes</a></li>
        <li><a href="https://docs.dask.org/en/latest/bag.html">Bags</a></li>
        <li><a href="https://docs.dask.org/en/latest/delayed.html">Delayed (custom)</a></li>
        <li><a href="https://docs.dask.org/en/latest/futures.html">Futures (real-time)</a></li>
        <li><a href="http://ml.dask.org">Machine Learning</a></li>
        <li><a href="https://xarray.pydata.org/en/latest/">XArray</a></li>
      </ul>
    </li>

    <li>
      <a href="https://docs.dask.org/en/latest/setup.html">Setup</a>
      <ul>
        <li><a href="https://docs.dask.org/en/latest/setup/single-machine.html"> Local </a></li>
        <li><a href="https://docs.dask.org/en/latest/setup/cloud.html"> Cloud </a></li>
        <li><a href="https://docs.dask.org/en/latest/setup/hpc.html"> HPC </a></li>
        <li><a href="https://kubernetes.dask.org/en/latest/"> Kubernetes </a></li>
        <li><a href="https://yarn.dask.org/en/latest/"> Hadoop / Yarn </a></li>
      </ul>
    </li>

    <li>
      <a>Community</a>
      <ul>
        <li><a href="http://docs.dask.org/en/latest/support.html">Ask for Help</a></li>
        <li><a href="https://github.com/dask">Github</a></li>
        <li><a href="https://stackoverflow.com/questions/tagged/dask">Stack Overflow</a></li>
        <li><a href="https://twitter.com/dask_dev">Twitter</a></li>
        <li><a href="https://blog.dask.org/"> Developer Blog </a></li>
      </ul>
    </li>
  </ul>

</nav>


  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> dask-ml
          

          
          </a>

          
            
            
              <div class="version">
                1.1.2.dev8+gfaaeff0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Use</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="preprocessing.html">Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="cross_validation.html">Cross Validation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Hyper Parameter Search</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#drop-in-replacements-for-scikit-learn">Drop-In Replacements for Scikit-Learn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#flexible-backends">Flexible Backends</a></li>
<li class="toctree-l3"><a class="reference internal" href="#works-well-with-dask-collections">Works Well With Dask Collections</a></li>
<li class="toctree-l3"><a class="reference internal" href="#avoid-repeated-work">Avoid Repeated Work</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#incremental-hyperparameter-optimization">Incremental Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="compose.html">Pipelines and Composite Estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="glm.html">Generalized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="joblib.html">Joblib</a></li>
<li class="toctree-l1"><a class="reference internal" href="meta-estimators.html">Parallel Meta-estimators</a></li>
<li class="toctree-l1"><a class="reference internal" href="incremental.html">Incremental Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="xgboost.html">XGBoost</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorflow.html">Tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules/api.html">API Reference</a></li>
</ul>
<p class="caption"><span class="caption-text">Develop</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Dask-ML Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="history.html">History</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">dask-ml</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Hyper Parameter Search</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/hyper-parameter-search.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hyper-parameter-search">
<h1>Hyper Parameter Search<a class="headerlink" href="#hyper-parameter-search" title="Permalink to this headline">¶</a></h1>
<p><em>Tools for performing hyperparameter optimization of Scikit-Learn API-compatible models using Dask</em>.</p>
<p>There are two kinds of hyperparameter optimization estimators
in Dask-ML. The appropriate one to use depends on the size of your dataset and
whether the underlying estimator implements the <cite>partial_fit</cite> method.</p>
<p>If your dataset is relatively small or the underlying estimator doesn’t implement
<code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>, you can use <a class="reference internal" href="modules/generated/dask_ml.model_selection.GridSearchCV.html#dask_ml.model_selection.GridSearchCV" title="dask_ml.model_selection.GridSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">dask_ml.model_selection.GridSearchCV</span></code></a> or
<a class="reference internal" href="modules/generated/dask_ml.model_selection.RandomizedSearchCV.html#dask_ml.model_selection.RandomizedSearchCV" title="dask_ml.model_selection.RandomizedSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">dask_ml.model_selection.RandomizedSearchCV</span></code></a>.
These are drop-in replacements for their scikit-learn counterparts, that should offer better performance and handling of Dask Arrays and DataFrames.
The underlying estimator will need to be able to train on each cross-validation split of the data.
See <a class="reference internal" href="#hyperparameter-drop-in"><span class="std std-ref">Drop-In Replacements for Scikit-Learn</span></a> for more.</p>
<p>If your data is large and the underlying estimator implements <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>, you can
Dask-ML’s <a class="reference internal" href="#hyperparameter-incremental"><span class="std std-ref">*incremental* hyperparameter optimizers</span></a>.</p>
<div class="section" id="drop-in-replacements-for-scikit-learn">
<span id="hyperparameter-drop-in"></span><h2>Drop-In Replacements for Scikit-Learn<a class="headerlink" href="#drop-in-replacements-for-scikit-learn" title="Permalink to this headline">¶</a></h2>
<p>Dask-ML implements GridSearchCV and RandomizedSearchCV.</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="(in scikit-learn v0.21.3)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.model_selection.GridSearchCV</span></code></a>(…[,&nbsp;…])</td>
<td>Exhaustive search over specified parameter values for an estimator.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="modules/generated/dask_ml.model_selection.GridSearchCV.html#dask_ml.model_selection.GridSearchCV" title="dask_ml.model_selection.GridSearchCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dask_ml.model_selection.GridSearchCV</span></code></a>(…[,&nbsp;…])</td>
<td>Exhaustive search over specified parameter values for an estimator.</td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV" title="(in scikit-learn v0.21.3)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.model_selection.RandomizedSearchCV</span></code></a>(…)</td>
<td>Randomized search on hyper parameters.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="modules/generated/dask_ml.model_selection.RandomizedSearchCV.html#dask_ml.model_selection.RandomizedSearchCV" title="dask_ml.model_selection.RandomizedSearchCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dask_ml.model_selection.RandomizedSearchCV</span></code></a>(…)</td>
<td>Randomized search on hyper parameters.</td>
</tr>
</tbody>
</table>
<p>The varians in Dask-ML implement many (but not all) of the same parameters,
and should be a drop-in replacement for the subset that they do implement.
In that case, why use Dask-ML’s versions?</p>
<ul class="simple">
<li><a class="reference internal" href="#flexible-backends"><span class="std std-ref">Flexible Backends</span></a>: Hyperparameter
optimization can be done in parallel using threads, processes, or distributed
across a cluster.</li>
<li><a class="reference internal" href="#works-with-dask-collections"><span class="std std-ref">Works well with Dask collections</span></a>. Dask
arrays, dataframes, and delayed can be passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</li>
<li><a class="reference internal" href="#avoid-repeated-work"><span class="std std-ref">Avoid repeated work</span></a>. Candidate estimators with
identical parameters and inputs will only be fit once. For
composite-estimators such as <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> this can be significantly more
efficient as it can avoid expensive repeated computations.</li>
</ul>
<p>Both scikit-learn’s and Dask-ML’s model selection meta-estimators can be used
with Dask’s <a class="reference internal" href="joblib.html#joblib"><span class="std std-ref">joblib backend</span></a>.</p>
<div class="section" id="flexible-backends">
<span id="id1"></span><h3>Flexible Backends<a class="headerlink" href="#flexible-backends" title="Permalink to this headline">¶</a></h3>
<p>Dask-ml can use any of the dask schedulers. By default the threaded
scheduler is used, but this can easily be swapped out for the multiprocessing
or distributed scheduler:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Distribute grid-search across a cluster</span>
<span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="n">scheduler_address</span> <span class="o">=</span> <span class="s1">&#39;127.0.0.1:8786&#39;</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">scheduler_address</span><span class="p">)</span>

<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="works-well-with-dask-collections">
<span id="works-with-dask-collections"></span><h3>Works Well With Dask Collections<a class="headerlink" href="#works-well-with-dask-collections" title="Permalink to this headline">¶</a></h3>
<p>Dask collections such as <code class="docutils literal notranslate"><span class="pre">dask.array</span></code>, <code class="docutils literal notranslate"><span class="pre">dask.dataframe</span></code> and
<code class="docutils literal notranslate"><span class="pre">dask.delayed</span></code> can be passed to <code class="docutils literal notranslate"><span class="pre">fit</span></code>. This means you can use dask to do
your data loading and preprocessing as well, allowing for a clean workflow.
This also allows you to work with remote data on a cluster without ever having
to pull it locally to your computer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dask.dataframe</span> <span class="k">as</span> <span class="nn">dd</span>

<span class="c1"># Load data from s3</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">dd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;s3://bucket-name/my-data-*.csv&#39;</span><span class="p">)</span>

<span class="c1"># Do some preprocessing steps</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;x2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1"># ...</span>

<span class="c1"># Pass to fit without ever leaving the cluster</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="avoid-repeated-work">
<span id="id2"></span><h3>Avoid Repeated Work<a class="headerlink" href="#avoid-repeated-work" title="Permalink to this headline">¶</a></h3>
<p>When searching over composite estimators like <code class="docutils literal notranslate"><span class="pre">sklearn.pipeline.Pipeline</span></code> or
<code class="docutils literal notranslate"><span class="pre">sklearn.pipeline.FeatureUnion</span></code>, Dask-ML will avoid fitting the same
estimator + parameter + data combination more than once. For pipelines with
expensive early steps this can be faster, as repeated work is avoided.</p>
<p>For example, given the following 3-stage pipeline and grid (modified from <a class="reference external" href="http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html">this
scikit-learn example</a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span> <span class="n">CountVectorizer</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s1">&#39;tfidf&#39;</span><span class="p">,</span> <span class="n">TfidfTransformer</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">())])</span>

<span class="n">grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;vect__ngram_range&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span>
        <span class="s1">&#39;tfidf__norm&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">],</span>
        <span class="s1">&#39;clf__alpha&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">]}</span>
</pre></div>
</div>
<p>the Scikit-Learn grid-search implementation looks something like (simplified):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ngram_range</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;vect__ngram_range&#39;</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">norm</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;tfidf__norm&#39;</span><span class="p">]:</span>
                <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;clf__alpha&#39;</span><span class="p">]:</span>
                        <span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="n">ngram_range</span><span class="p">)</span>
                        <span class="n">X2</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                        <span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
                        <span class="n">X3</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                        <span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
                        <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X3</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X3</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">choose_best_parameters</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
<p>As a directed acyclic graph, this might look like:</p>
<div class="figure align-center">
<img alt="&quot;scikit-learn grid-search directed acyclic graph&quot;" src="_images/unmerged_grid_search_graph.svg" /></div>
<p>In contrast, the dask version looks more like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ngram_range</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;vect__ngram_range&#39;</span><span class="p">]:</span>
        <span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="n">ngram_range</span><span class="p">)</span>
        <span class="n">X2</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">norm</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;tfidf__norm&#39;</span><span class="p">]:</span>
                <span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
                <span class="n">X3</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;clf__alpha&#39;</span><span class="p">]:</span>
                        <span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
                        <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X3</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                        <span class="n">scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X3</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">choose_best_parameters</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
<p>With a corresponding directed acyclic graph:</p>
<div class="figure align-center">
<img alt="&quot;Dask-ML grid-search directed acyclic graph&quot;" src="_images/merged_grid_search_graph.svg" /></div>
<p>Looking closely, you can see that the Scikit-Learn version ends up fitting
earlier steps in the pipeline multiple times with the same parameters and data.
Due to the increased flexibility of Dask over Joblib, we’re able to merge these
tasks in the graph and only perform the fit step once for any
parameter/data/estimator combination. For pipelines that have relatively
expensive early steps, this can be a big win when performing a grid search.</p>
</div>
</div>
<div class="section" id="incremental-hyperparameter-optimization">
<span id="hyperparameter-incremental"></span><h2>Incremental Hyperparameter Optimization<a class="headerlink" href="#incremental-hyperparameter-optimization" title="Permalink to this headline">¶</a></h2>
<p>The second category of hyperparameter optimization uses <em>incremental</em>
hyperparameter optimization. These should be used when your full dataset doesn’t
fit in memory on a single machine.</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="modules/generated/dask_ml.model_selection.IncrementalSearchCV.html#dask_ml.model_selection.IncrementalSearchCV" title="dask_ml.model_selection.IncrementalSearchCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dask_ml.model_selection.IncrementalSearchCV</span></code></a>(…)</td>
<td>Incrementally search for hyper-parameters on models that support partial_fit</td>
</tr>
</tbody>
</table>
<p>Broadly speaking, incremental optimization starts with a batch of models (underlying
estimators and hyperparameter combinations) and repeatedly calls the underlying estimator’s
<code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> method with batches of data.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These estimators require the optional <code class="docutils literal notranslate"><span class="pre">distributed</span></code> library.</p>
</div>
<p>Here’s an example training on a “large” dataset (a Dask array) with the
<code class="docutils literal notranslate"><span class="pre">IncrementalSearchCV</span></code>.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [1]: </span><span class="kn">from</span> <span class="nn">dask.distributed</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="gp">In [2]: </span><span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">()</span>

<span class="gp">In [3]: </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="gp">In [4]: </span><span class="kn">from</span> <span class="nn">dask_ml.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>

<span class="gp">In [5]: </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">5000000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">   ...: </span>                           <span class="n">chunks</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">   ...: </span>
</pre></div>
</div>
<p>Our underlying estimator is an SGDClassifier. We specify a few parameters
common to each clone of the estimator:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [6]: </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>

<span class="gp">In [7]: </span><span class="n">model</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;elasticnet&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>We also define the distribution of parameters from which we will sample:</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [8]: </span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
<span class="gp">   ...: </span>          <span class="s1">&#39;l1_ratio&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
<span class="gp">   ...: </span>          <span class="s1">&#39;average&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]}</span>
<span class="gp">   ...: </span>
</pre></div>
</div>
<p>Finally we create many random models in this parameter space and
train-and-score them until we find the best one.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [9]: </span><span class="kn">from</span> <span class="nn">dask_ml.model_selection</span> <span class="kn">import</span> <span class="n">IncrementalSearchCV</span>

<span class="gp">In [10]: </span><span class="n">search</span> <span class="o">=</span> <span class="n">IncrementalSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">In [11]: </span><span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gh">Out[11]: </span><span class="go"></span>
<span class="go">IncrementalSearchCV(decay_rate=1.0,</span>
<span class="go">                    estimator=SGDClassifier(alpha=0.0001, average=False,</span>
<span class="go">                                            class_weight=None,</span>
<span class="go">                                            early_stopping=False, epsilon=0.1,</span>
<span class="go">                                            eta0=0.0, fit_intercept=True,</span>
<span class="go">                                            l1_ratio=0.15,</span>
<span class="go">                                            learning_rate=&#39;optimal&#39;,</span>
<span class="go">                                            loss=&#39;hinge&#39;, max_iter=1000,</span>
<span class="go">                                            n_iter_no_change=5, n_jobs=None,</span>
<span class="go">                                            penalty=&#39;elasticnet&#39;, power_t=0.5,</span>
<span class="go">                                            random_state=0, shuffle=True,</span>
<span class="go">                                            tol=0.001, validation_fraction=0.1,</span>
<span class="go">                                            verbose=0, warm_start=False),</span>
<span class="go">                    max_iter=100, n_initial_parameters=10,</span>
<span class="go">                    param_distribution=None, patience=False, random_state=0,</span>
<span class="go">                    scores_per_fit=1, scoring=None, test_size=None, tol=0.001)</span>
</pre></div>
</div>
<p>Note that when you do post-fit tasks like <code class="docutils literal notranslate"><span class="pre">search.score</span></code>, the underlying
estimator’s score method is used. If that is unable to handle a
larger-than-memory Dask Array, you’ll exhaust your machines memory. If you plan
to use post-estimation features like scoring or prediction, we recommend using
<a class="reference internal" href="modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit" title="dask_ml.wrappers.ParallelPostFit"><code class="xref py py-class docutils literal notranslate"><span class="pre">dask_ml.wrappers.ParallelPostFit</span></code></a>.</p>
<div class="highlight-ipython notranslate"><div class="highlight"><pre><span></span><span class="gp">In [12]: </span><span class="kn">from</span> <span class="nn">dask_ml.wrappers</span> <span class="kn">import</span> <span class="n">ParallelPostFit</span>

<span class="gp">In [13]: </span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;estimator__alpha&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
<span class="gp">   ....: </span>          <span class="s1">&#39;estimator__l1_ratio&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
<span class="gp">   ....: </span>          <span class="s1">&#39;estimator__average&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]}</span>
<span class="gp">   ....: </span>

<span class="gp">In [14]: </span><span class="n">model</span> <span class="o">=</span> <span class="n">ParallelPostFit</span><span class="p">(</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
<span class="gp">   ....: </span>                                      <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span>
<span class="gp">   ....: </span>                                      <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="gp">   ....: </span>

<span class="gp">In [15]: </span><span class="n">search</span> <span class="o">=</span> <span class="n">IncrementalSearchCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">In [16]: </span><span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gh">Out[16]: </span><span class="go"></span>
<span class="go">IncrementalSearchCV(decay_rate=1.0,</span>
<span class="go">                    estimator=ParallelPostFit(estimator=SGDClassifier(alpha=0.0001,</span>
<span class="go">                                                                      average=False,</span>
<span class="go">                                                                      class_weight=None,</span>
<span class="go">                                                                      early_stopping=False,</span>
<span class="go">                                                                      epsilon=0.1,</span>
<span class="go">                                                                      eta0=0.0,</span>
<span class="go">                                                                      fit_intercept=True,</span>
<span class="go">                                                                      l1_ratio=0.15,</span>
<span class="go">                                                                      learning_rate=&#39;optimal&#39;,</span>
<span class="go">                                                                      loss=&#39;hinge&#39;,</span>
<span class="go">                                                                      max_iter=1000,</span>
<span class="go">                                                                      n_iter_no_change=5,</span>
<span class="go">                                                                      n_jobs=None,</span>
<span class="go">                                                                      penalty=&#39;elasticnet&#39;,</span>
<span class="go">                                                                      power_t=0.5,</span>
<span class="go">                                                                      random_state=0,</span>
<span class="go">                                                                      shuffle=True,</span>
<span class="go">                                                                      tol=0.001,</span>
<span class="go">                                                                      validation_fraction=0.1,</span>
<span class="go">                                                                      verbose=0,</span>
<span class="go">                                                                      warm_start=False),</span>
<span class="go">                                              scoring=None),</span>
<span class="go">                    max_iter=100, n_initial_parameters=10,</span>
<span class="go">                    param_distribution=None, patience=False, random_state=0,</span>
<span class="go">                    scores_per_fit=1, scoring=None, test_size=None, tol=0.001)</span>

<span class="gp">In [17]: </span><span class="n">search</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gh">Out[17]: </span><span class="go">0.5864312</span>
</pre></div>
</div>
<p>Note that the parameter names include the <code class="docutils literal notranslate"><span class="pre">estimator__</span></code> prefix,
as we’re tuning the hyperparameters of the <code class="docutils literal notranslate"><span class="pre">SGDClassifier</span></code> that’s
underlying the <code class="docutils literal notranslate"><span class="pre">ParallelPostFit</span></code>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="compose.html" class="btn btn-neutral float-right" title="Pipelines and Composite Estimators" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cross_validation.html" class="btn btn-neutral float-left" title="Cross Validation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Dask developers

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>